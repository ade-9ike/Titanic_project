{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Titanic Survival Prediction Project</h1>\n\n<h2>Introduction</h2>\n\nThis project involves predicting the survival status of passengers aboard the Titanic, whether they \"Survived\" or \"Did Not Survive\" the disaster. This is a classic binary classification problem in supervised machine learning, where we aim to predict one of two distinct outcomes: \"Survived\" (represented as \"1\") or \"Did not Survive\" (represented as \"0\").\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Exploratory Data Analysis (EDA)</h2>\n\nBefore constructing our predictive model, we conducted an exploratory data analysis (EDA) to achieve the following objectives:\n\n* Gain insights into the dataset.\n* Clean and preprocess the data to make it suitable for model building.\n\nIn the EDA phase, we examined various aspects of the data to better understand its characteristics and potential patterns. This analysis will inform our subsequent steps in constructing a predictive model for Titanic survival.","metadata":{}},{"cell_type":"markdown","source":"<h2>DATA CLEANING</h2>","metadata":{}},{"cell_type":"code","source":"#Load libraries into code environment\n\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Data Loading and Initial Assessment</h3>\n\nIn this stage of the project, we loaded the training and testing datasets into our code environment. We utilized the .info() method to gain a preliminary understanding of the data, including its basic characteristics and the presence of missing values.\n\n<h3>Identifying Missing Data</h3>\n\nUpon inspecting the datasets, we observed that both the training and testing datasets exhibit missing values specifically in the \"Age\" and \"Cabin\" columns. The presence of missing data is a crucial aspect to address during data preprocessing to ensure the quality of our model and predictions.","metadata":{}},{"cell_type":"code","source":"#Load test and train datasets into code environment\n\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n#Preview and get some info about the dataset\ntest.info()\ntrain.info()\n\ntest.Age.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Data Preprocessing: Handling Missing Data</h3>\n\nTo address the missing data in the \"Age\" column of the training dataset, we applied the following data imputation technique:\n\n<h3>Imputing Missing Age Values</h3>\n\nWe employed the Simple Imputer method from the sklearn library to fill in the missing values in the \"Age\" column. Specifically, we imputed these missing data points with the average age of the passengers. This approach helps ensure that the dataset remains complete and ready for further analysis and model building.","metadata":{}},{"cell_type":"code","source":"#Fill missing values in Age column of train dataset with mean of the column\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\n\nage_column_train = train[['Age']]\n\n\nimputer.fit(age_column_train)\n\n\ntrain['Age'] = imputer.transform(age_column_train)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Feature Engineering</h2>\n\nIn the feature engineering phase, we aim to prepare our dataset for model building by selecting and transforming relevant features. Here are the key steps we took:\n\n<h3>Identifying the Target Variable</h3>\nWe identified the target variable for our prediction task as the 'Survived' column in the training dataset.\n\n<h3>Feature Selection</h3>\nTo enhance the quality of our model, we conducted feature selection by excluding columns that are not significant for predicting the target variable. Notably, we dropped the 'Cabin' column due to a large number of missing values (687 out of 891 datapoints).\n\n<h3>Handling Categorical and Ordinal Data</h3>\nTo make the dataset compatible with various machine learning algorithms and to improve model performance, we converted categorical and ordinal data into numeric format. This transformation ensures that our model can effectively utilize these variables for prediction.\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"#Drop the columns that are less significant\n\ntrain_rev = train.drop(['Name', 'Cabin', 'Ticket', 'PassengerId'], axis=1)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert column with ordinal data to numeric data using Label Encoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain_rev['Pclass'] = le.fit_transform(train_rev['Pclass'])\n\n\n#Convert columns with categorical data to numeric data using One Hot Encoder\ntrain_rev = pd.get_dummies(train_rev, columns=['Sex', 'Embarked'])\n\n\ntrain_rev.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>More Feature Engineering</h3>\n\nAfter successfully converting all relevant data to numeric format, we now have a dataset with 11 features remaining.\n\n<h3>Correlation Analysis</h3>\n\nTo better understand the relationships between these features and our target variable, \"Survived,\" we conducted correlation analysis. This analysis provides insights into how each feature may influence the likelihood of survival and is visualized to facilitate our understanding.","metadata":{}},{"cell_type":"code","source":"#Check relationship between features and target variable using correlation analysis\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntarget_variable = 'Survived'\ncorrelation_matrix = train_rev.corr()\ntarget_correlations = correlation_matrix[target_variable]\n\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='cividis', fmt=\".2f\")\nplt.title(f'Correlation Heatmap (Target Variable: {target_variable})')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Correlation Analysis Results</h3>\n\nUpon performing correlation analysis, we observed that some of the strongest correlations with the target variable, \"Survived,\" are found within the gender-related features.\n\n<h3>Feature Selection Based on Correlation</h3>\n\nIn light of these findings, we made informed decisions regarding feature selection. Specifically, we opted to drop some of the features with the lowest correlation to the target variable. This step was taken because these features showed little to no relationship with survival, and retaining them would not significantly contribute to the predictive power of our model.","metadata":{}},{"cell_type":"code","source":"#Drop features with the lowest correlation to the target variable\n\ntrain_rev = train_rev.drop(['Parch','Embarked_C', 'Embarked_Q', 'Embarked_S'], axis=1)\n\ntrain_rev.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Model Training</h2>\n\nIn this phase of the project, we have carefully selected three models suitable for binary classification problems. These models will be trained using the preprocessed training dataset, and their performance will be rigorously evaluated.\n\n<h3>Model Selection</h3>\nWe chose three distinct models, each known for its effectiveness in binary classification tasks. These models will serve as candidates for predicting the survival outcome:\n\n* Logistic regression\n* Decision Tree Classifier\n* Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"#Import the necessary sklearn libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n\nX = train_rev.drop(\"Survived\", axis=1)\ny = train_rev[['Survived']]\n\n#Split the train dataset\nX_train, X_test ,y_train ,y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#Initializing models\nmodels = [\n    (\"Logistic Regression\", LogisticRegression()),\n    (\"Random Forest\", RandomForestClassifier()),\n    (\"Decision Tree\", DecisionTreeClassifier())\n        ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Model Evaluation</h2>\n\nIn this section, we comprehensively evaluate the performance of our models using three key metrics: \n\n<h3>Performance Metrics</h3>\n\nTo assess the performance of these models, we have established three key metrics:\n\n* Accuracy: Measures the overall correctness of predictions.\n* Precision: Evaluates the ability of the model to make accurate positive predictions.\n* Recall: Measures the ability of the model to correctly identify actual positive instances.\n\n<h3>Confusion Matrix</h3>\nWe utilize confusion matrices to visually interpret the performance of each model. These matrices provide a comprehensive breakdown of metric scores, giving us valuable insights into the strengths and weaknesses of each model. Ultimately, this analysis will guide our selection of the model that best suits our prediction task.\nThese matrices offer a visual breakdown of performance, including true positives, true negatives, false positives, and false negatives.\n\n<h3>Cross Validation</h3>\n\nTo enhance the reliability and generalizability of our models, we employ 5-fold cross-validation. This approach involves training and evaluating the models multiple times on different data subsets. By using our selected metrics as the scoring criteria during cross-validation, we not only mitigate overfitting but also maximize the utilization of our dataset, ultimately ensuring the robustness of our models.","metadata":{}},{"cell_type":"code","source":"#Metric 1 - Accuracy\n\nfor model_name,model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    print(f\"{model_name} Accuracy: {accuracy}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metric 2 - Precision\n\nfor model_name,model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    precision = precision_score(y_test, y_pred)\n    \n    print(f\"{model_name} Precision: {precision}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Metric 3 - Recall\n\nfor model_name,model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    recall = recall_score(y_test, y_pred)\n    \n    print(f\"{model_name} Recall: {recall}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model_name,model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    cm = confusion_matrix(y_test, y_pred)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(f\"Confusion Matrix for {model_name}\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scoring = ['accuracy', 'precision', 'recall']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform cross-validation and evaluate models\nfor model_name, model in models:\n    # Perform cross-validation (5-fold in this example)\n    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring= scoring)\n    \n    # Fit the model on the full training data\n    model.fit(X_train, y_train)\n    \n    # Make predictions on the test data\n    y_pred = model.predict(X_test)\n    \n    # Calculate evaluation metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    \n    for metric in scoring:\n        scores = cv_results[f\"test_{metric}\"]\n        print(f\"{model_name} - {metric.capitalize()} Scores: {scores}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Observation and Model Selection</h2>\n\nAfter conducting cross-validation, we observed a considerable reduction in the variance of model performance. Among the three models we employed for training our dataset, we noticed a notable observation within the Confusion Matrix of the Random Forest model:\n\n<h3>Random Forest Model Performance</h3>\n\nThe Random Forest model exhibited the highest number of True Positives.\nThis model demonstrated some of the best scores when evaluated using our selected metrics, including Accuracy, Precision, and Recall.\n\n<h3>Model Selection</h3>\n\nTaking these observations into account, we have made the decision to select the Random Forest Classifier model as our preferred choice. This model will be used for validating our test data and making predictions, as it has shown promising performance during our evaluation phase.\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<h2>Inference</h2>\n\nIn this phase, we apply our trained Random Forest model to make predictions on a new dataset, the Test data. Before proceeding with predictions, it is crucial to ensure that the Test data is preprocessed and formatted consistently with our training dataset.\n\n<h3>Data Preprocessing</h3>\n\nTo prepare the Test data for prediction, we perform the following steps:\n\n* Data Cleaning: We address any missing values and ensure that the dataset is free of inconsistencies or outliers.\n* Data Formatting: The Test data is formatted to match the structure of our training dataset, ensuring that it contains the same features and data types.\n\n<h3>Prediction</h3>\nOnce the Test data is appropriately cleaned and formatted, we utilize our previously trained Random Forest model to make predictions. These predictions will provide insights into the likelihood of survival for passengers in the Test dataset, based on the patterns and information learned from our training data.","metadata":{}},{"cell_type":"code","source":"#Check the test dataset\n\n#Fill missing data in Age column with mean\nage_column_test = test[['Age']]\nimputer.fit(age_column_test)\ntest['Age'] = imputer.transform(age_column_test)\n\n#Drop columns that were dropped in train dataset\ntest_rev = test.drop(['Name', 'Cabin', 'Ticket', 'Embarked', 'Parch','PassengerId'], axis=1)\n\n#Transform ordinal data to numeric data\ntest_rev['Pclass'] = le.fit_transform(test_rev['Pclass'])\n#Convert columns with categorical data to numeric data using Label Encoder\ntest_rev = pd.get_dummies(test_rev, columns=['Sex']) \n\n\n#Fill the missing data in the fare column with the most frequent data\n\nimputer2 = SimpleImputer(strategy='most_frequent')\nfare_column_test = test_rev[['Fare']]\nimputer2.fit(fare_column_test)\ntest_rev['Fare'] = imputer2.transform(fare_column_test)\n\ntest_rev.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model_name, model in models:\n    if model_name == \"Random Forest\":\n        random_forest_model = model\n        \n        y_train = y_train.values.ravel()\n        \n        random_forest_model.fit(X_train, y_train)\n        \n        predictions = random_forest_model.predict(test_rev)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})\n\n\npredictions.to_csv('submissions.csv', index=False)\n\npredictions[predictions['Survived'] == 1].sum()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# To download the file in Jupyter Notebook\n\nfrom IPython.display import FileLink\nFileLink('submissions.csv')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Prediction and Result Storage</h2>\n\nFollowing the application of our trained Random Forest model to the Test dataset, predictions were generated and stored in the 'Survived' column of a predictions dataframe.\n\nOur model successfully predicted that 138 passengers survived the Titanic disaster.\n\nTo facilitate submission and further analysis, the predictions dataframe has been converted and saved as a CSV file named 'submissions.csv'. This file contains the predicted survival outcomes for the passengers in the Test dataset, allowing for easy sharing and assessment of our model's performance.","metadata":{}}]}